{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3FnyCkw9v0nz"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "from glob import glob\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(water_quality := (\n",
        "    pl.scan_csv(\n",
        "        'data/MinneMUDAC_raw_files/water_quality_and_parcel_summaries_2004_to_2015.csv',\n",
        "        has_header=True,\n",
        "        separator=',',\n",
        "        infer_schema_length=10000,\n",
        "        schema_overrides={'pH': pl.Float64}\n",
        "    )\n",
        "    .limit(100)\n",
        "    .collect(\n",
        "    )\n",
        ")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "T1x91SwEwOgY",
        "outputId": "063baa1e-8c0e-4cff-b7bd-70693ff9f3d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (100, 30)\n",
              "┌─────┬─────────────┬──────┬─────────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n",
              "│     ┆ Monit_MAP_C ┆ Year ┆ LAKE_NAME   ┆ … ┆ Percentage_ ┆ Percentage_ ┆ Percentage ┆ Percentage │\n",
              "│ --- ┆ ODE1        ┆ ---  ┆ ---         ┆   ┆ Water_Heati ┆ Electric_He ┆ _Other_Hea ┆ _No_Heatin │\n",
              "│ i64 ┆ ---         ┆ i64  ┆ str         ┆   ┆ ng          ┆ ating       ┆ ting       ┆ g          │\n",
              "│     ┆ str         ┆      ┆             ┆   ┆ ---         ┆ ---         ┆ ---        ┆ ---        │\n",
              "│     ┆             ┆      ┆             ┆   ┆ f64         ┆ f64         ┆ f64        ┆ f64        │\n",
              "╞═════╪═════════════╪══════╪═════════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n",
              "│ 0   ┆ 02000500-01 ┆ 2014 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ 85.325639  ┆ 14.674361  │\n",
              "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
              "│ 1   ┆ 02000500-01 ┆ 2013 ┆ George      ┆ … ┆ 1.154163    ┆ 2.802968    ┆ 1.154163   ┆ 0.0        │\n",
              "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
              "│ 2   ┆ 02000500-01 ┆ 2012 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ 0.0        ┆ 0.0        │\n",
              "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
              "│ 3   ┆ 02000500-01 ┆ 2011 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ 0.0        ┆ 0.0        │\n",
              "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
              "│ 4   ┆ 02000500-01 ┆ 2010 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ 0.0        ┆ 0.0        │\n",
              "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
              "│ …   ┆ …           ┆ …    ┆ …           ┆ … ┆ …           ┆ …           ┆ …          ┆ …          │\n",
              "│ 95  ┆ 19002200-01 ┆ 2007 ┆ Long Lake   ┆ … ┆ 0.0         ┆ 0.0         ┆ 0.0        ┆ 0.0        │\n",
              "│ 96  ┆ 19002200-01 ┆ 2006 ┆ Long Lake   ┆ … ┆ 0.0         ┆ 0.0         ┆ 0.0        ┆ 0.0        │\n",
              "│ 97  ┆ 19002200-01 ┆ 2005 ┆ Long Lake   ┆ … ┆ 0.0         ┆ 0.0         ┆ 0.0        ┆ 0.0        │\n",
              "│ 98  ┆ 19002200-01 ┆ 2004 ┆ Long Lake   ┆ … ┆ 0.0         ┆ 0.0         ┆ 0.0        ┆ 0.0        │\n",
              "│ 99  ┆ 19002300-01 ┆ 2014 ┆ Farquar     ┆ … ┆ 0.492459    ┆ 0.0         ┆ 0.061557   ┆ 0.0        │\n",
              "│     ┆             ┆      ┆ Lake        ┆   ┆             ┆             ┆            ┆            │\n",
              "└─────┴─────────────┴──────┴─────────────┴───┴─────────────┴─────────────┴────────────┴────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (100, 30)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>Monit_MAP_CODE1</th><th>Year</th><th>LAKE_NAME</th><th>Mean_Secchi_Depth_Result</th><th>Mean_Phosporus_Result</th><th>Mean_EMV_Total</th><th>STD_EMV_Total</th><th>Mean_Sale_Value</th><th>STD_Sale_Value</th><th>Mean_Total_Tax</th><th>STD_Total_Tax</th><th>Mean_Garage_Size</th><th>STD_Garage_Size</th><th>Mean_Fin_SQ_FT</th><th>STD_Fin_SQ_FT</th><th>Percentage_Yes_Basement</th><th>Percentage_Yes_Garage</th><th>Percentage_Yes_Tax_Exempt</th><th>Percentage_Air_Cooling</th><th>Percentage_AC_Cooling</th><th>Percentage_Central_Cooling</th><th>Percentage_Other_Cooling</th><th>Percentage_No_Cooling</th><th>Percentage_Air_Heating</th><th>Percentage_Space_Heating</th><th>Percentage_Water_Heating</th><th>Percentage_Electric_Heating</th><th>Percentage_Other_Heating</th><th>Percentage_No_Heating</th></tr><tr><td>i64</td><td>str</td><td>i64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>&quot;02000500-01&quot;</td><td>2014</td><td>&quot;George Watch Lake&quot;</td><td>0.716667</td><td>0.108778</td><td>215637.592745</td><td>459023.922281</td><td>122485.615829</td><td>183414.899771</td><td>3313.685903</td><td>17837.814516</td><td>null</td><td>null</td><td>1849.473207</td><td>8220.487298</td><td>70.486397</td><td>0.0</td><td>7.914262</td><td>0.0</td><td>0.0</td><td>0.0</td><td>2.308326</td><td>97.691674</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>85.325639</td><td>14.674361</td></tr><tr><td>1</td><td>&quot;02000500-01&quot;</td><td>2013</td><td>&quot;George Watch Lake&quot;</td><td>0.365</td><td>0.3105</td><td>196764.633141</td><td>454192.377508</td><td>120601.723001</td><td>190129.461399</td><td>3319.081616</td><td>18199.952917</td><td>null</td><td>null</td><td>1841.112119</td><td>8221.467234</td><td>69.991756</td><td>0.0</td><td>7.914262</td><td>0.824402</td><td>0.0</td><td>0.0</td><td>1.154163</td><td>0.0</td><td>79.060181</td><td>0.824402</td><td>1.154163</td><td>2.802968</td><td>1.154163</td><td>0.0</td></tr><tr><td>2</td><td>&quot;02000500-01&quot;</td><td>2012</td><td>&quot;George Watch Lake&quot;</td><td>0.359</td><td>0.2649</td><td>200414.333057</td><td>480355.968668</td><td>118315.128418</td><td>190091.029827</td><td>3460.064623</td><td>19346.923479</td><td>null</td><td>null</td><td>1811.576636</td><td>8223.512958</td><td>70.173985</td><td>0.0</td><td>8.119304</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>3</td><td>&quot;02000500-01&quot;</td><td>2011</td><td>&quot;George Watch Lake&quot;</td><td>0.973333</td><td>0.119417</td><td>216297.932175</td><td>536941.029067</td><td>111218.812242</td><td>190312.451914</td><td>3459.313482</td><td>19320.28833</td><td>null</td><td>null</td><td>1780.282051</td><td>8215.092878</td><td>76.840364</td><td>0.0</td><td>8.271299</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>4</td><td>&quot;02000500-01&quot;</td><td>2010</td><td>&quot;George Watch Lake&quot;</td><td>0.493333</td><td>0.173</td><td>222789.430223</td><td>559656.083477</td><td>110765.119736</td><td>190448.272924</td><td>3384.645747</td><td>18646.626107</td><td>null</td><td>null</td><td>1760.876135</td><td>8210.519477</td><td>75.887696</td><td>0.0</td><td>8.092486</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>95</td><td>&quot;19002200-01&quot;</td><td>2007</td><td>&quot;Long Lake&quot;</td><td>1.733766</td><td>0.102672</td><td>302888.441533</td><td>471157.778953</td><td>141962.587555</td><td>127724.729722</td><td>3097.440955</td><td>3068.087373</td><td>null</td><td>null</td><td>2275.401657</td><td>2624.866538</td><td>0.0</td><td>0.0</td><td>2.273165</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>96</td><td>&quot;19002200-01&quot;</td><td>2006</td><td>&quot;Long Lake&quot;</td><td>1.459746</td><td>0.111475</td><td>298946.734733</td><td>459972.909725</td><td>137663.425544</td><td>126084.446256</td><td>3015.460412</td><td>2593.59527</td><td>null</td><td>null</td><td>2266.360239</td><td>2642.997239</td><td>0.0</td><td>0.0</td><td>2.273165</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>97</td><td>&quot;19002200-01&quot;</td><td>2005</td><td>&quot;Long Lake&quot;</td><td>1.463409</td><td>0.082091</td><td>276705.113856</td><td>416702.275065</td><td>132619.693941</td><td>122315.844073</td><td>2491.457545</td><td>1530.016612</td><td>null</td><td>null</td><td>2247.141258</td><td>2611.387698</td><td>0.0</td><td>0.0</td><td>2.315708</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>98</td><td>&quot;19002200-01&quot;</td><td>2004</td><td>&quot;Long Lake&quot;</td><td>1.364328</td><td>0.0875625</td><td>255819.364774</td><td>406595.520932</td><td>126841.389028</td><td>125339.552401</td><td>2384.751299</td><td>2484.467371</td><td>null</td><td>null</td><td>2227.763041</td><td>2604.50713</td><td>0.0</td><td>0.0</td><td>2.309913</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>99</td><td>&quot;19002300-01&quot;</td><td>2014</td><td>&quot;Farquar Lake&quot;</td><td>0.754</td><td>0.1256</td><td>317861.834411</td><td>221105.149397</td><td>241022.402278</td><td>174027.83847</td><td>3896.223761</td><td>1780.27379</td><td>558.94152</td><td>269.116157</td><td>2807.499846</td><td>1079.842032</td><td>0.0</td><td>96.152662</td><td>1.292705</td><td>95.721761</td><td>0.0</td><td>0.0</td><td>0.554017</td><td>0.0</td><td>95.721761</td><td>0.0</td><td>0.492459</td><td>0.0</td><td>0.061557</td><td>0.0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acfb64d3"
      },
      "source": [
        "# Task 1\n",
        "We want to extract 30% of the lakes to be used as a validation set, with the remaining lakes separated into a train/test split (again use 70%/30% split).In summary we should have a random split with\n",
        "1. 30% of the lakes marked as \"Validation\",\n",
        "2. 50% of the lakes (~70% of 70%) marked as \"Training\", and\n",
        "3. 20% of the lakes (~30% of 70%) marked as \"Test\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dfe28e8",
        "outputId": "9db5bee4-8235-43c5-a50d-0239286729ef"
      },
      "source": [
        "unique_lakes = water_quality['LAKE_NAME'].unique().to_list()\n",
        "print(f\"Number of unique lakes: {len(unique_lakes)}\")\n",
        "print(\"First 5 unique lake names:\")\n",
        "print(unique_lakes[:5])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique lakes: 10\n",
            "First 5 unique lake names:\n",
            "['Alimagnet Lake', 'Riley Lake', 'George Watch Lake', 'Long Lake', 'Bavaria Lake']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a376f8ec"
      },
      "source": [
        "## Split Lakes into Validation, Training, and Test Sets\n",
        "\n",
        "### Subtask:\n",
        "Randomly assign each unique lake to one of three groups: 'Validation' (30% of lakes), 'Training' (50% of lakes), or 'Test' (20% of lakes). This ensures that lakes are not duplicated across different sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf02e259",
        "outputId": "6ba97064-a351-405e-852b-be4e8718ad9a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "shuffled_lakes = np.array(unique_lakes)\n",
        "np.random.shuffle(shuffled_lakes)\n",
        "\n",
        "# Split into Test (20%) and Training/Validation (80%)\n",
        "train_val_lakes, test_lakes = train_test_split(shuffled_lakes, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split Training/Validation (80%) into Training (50% of total) and Validation (30% of total)\n",
        "# Since train_val_lakes is 80% of the total, 50% of total is 50/80 = 0.625 of train_val_lakes\n",
        "# And 30% of total is 30/80 = 0.375 of train_val_lakes\n",
        "train_lakes, val_lakes = train_test_split(train_val_lakes, test_size=0.375, random_state=42) # 0.375 of 80% = 30% of total\n",
        "\n",
        "print(f\"Number of unique lakes: {len(unique_lakes)}\")\n",
        "print(f\"Number of lakes in Training set: {len(train_lakes)}\")\n",
        "print(f\"Number of lakes in Validation set: {len(val_lakes)}\")\n",
        "print(f\"Number of lakes in Test set: {len(test_lakes)}\")\n",
        "\n",
        "print(f\"\\nTraining set proportion: {len(train_lakes) / len(unique_lakes):.2f}\")\n",
        "print(f\"Validation set proportion: {len(val_lakes) / len(unique_lakes):.2f}\")\n",
        "print(f\"Test set proportion: {len(test_lakes) / len(unique_lakes):.2f}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique lakes: 10\n",
            "Number of lakes in Training set: 5\n",
            "Number of lakes in Validation set: 3\n",
            "Number of lakes in Test set: 2\n",
            "\n",
            "Training set proportion: 0.50\n",
            "Validation set proportion: 0.30\n",
            "Test set proportion: 0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cabbd9e",
        "outputId": "0f3b881c-2438-4068-d135-69d5a3212fad"
      },
      "source": [
        "lake_to_split_group = {}\n",
        "for lake in train_lakes:\n",
        "    lake_to_split_group[lake] = 'Training'\n",
        "for lake in val_lakes:\n",
        "    lake_to_split_group[lake] = 'Validation'\n",
        "for lake in test_lakes:\n",
        "    lake_to_split_group[lake] = 'Test'\n",
        "\n",
        "\n",
        "split_group_df = pl.DataFrame({\n",
        "    'LAKE_NAME': list(lake_to_split_group.keys()),\n",
        "    'Split_Group': list(lake_to_split_group.values())\n",
        "})\n",
        "\n",
        "\n",
        "water_quality = water_quality.join(split_group_df, on='LAKE_NAME', how='left')\n",
        "\n",
        "print(water_quality.head())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 32)\n",
            "┌─────┬─────────────┬──────┬─────────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n",
            "│     ┆ Monit_MAP_C ┆ Year ┆ LAKE_NAME   ┆ … ┆ Percentage_ ┆ Percentage_ ┆ Split_Grou ┆ Split_Grou │\n",
            "│ --- ┆ ODE1        ┆ ---  ┆ ---         ┆   ┆ Other_Heati ┆ No_Heating  ┆ p          ┆ p_right    │\n",
            "│ i64 ┆ ---         ┆ i64  ┆ str         ┆   ┆ ng          ┆ ---         ┆ ---        ┆ ---        │\n",
            "│     ┆ str         ┆      ┆             ┆   ┆ ---         ┆ f64         ┆ str        ┆ str        │\n",
            "│     ┆             ┆      ┆             ┆   ┆ f64         ┆             ┆            ┆            │\n",
            "╞═════╪═════════════╪══════╪═════════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n",
            "│ 0   ┆ 02000500-01 ┆ 2014 ┆ George      ┆ … ┆ 85.325639   ┆ 14.674361   ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "│ 1   ┆ 02000500-01 ┆ 2013 ┆ George      ┆ … ┆ 1.154163    ┆ 0.0         ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "│ 2   ┆ 02000500-01 ┆ 2012 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "│ 3   ┆ 02000500-01 ┆ 2011 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "│ 4   ┆ 02000500-01 ┆ 2010 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "└─────┴─────────────┴──────┴─────────────┴───┴─────────────┴─────────────┴────────────┴────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27ca0860",
        "outputId": "e81f45ea-c767-43b4-cdad-345081de17d4"
      },
      "source": [
        "lake_to_split_group = {}\n",
        "for lake in train_lakes:\n",
        "    lake_to_split_group[lake] = 'Training'\n",
        "for lake in val_lakes:\n",
        "    lake_to_split_group[lake] = 'Validation'\n",
        "for lake in test_lakes:\n",
        "    lake_to_split_group[lake] = 'Test'\n",
        "\n",
        "split_group_df = pl.DataFrame({\n",
        "    'LAKE_NAME': list(lake_to_split_group.keys()),\n",
        "    'Split_Group': list(lake_to_split_group.values())\n",
        "})\n",
        "\n",
        "water_quality = water_quality.join(split_group_df, on='LAKE_NAME', how='left')\n",
        "\n",
        "print(water_quality.head())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 32)\n",
            "┌─────┬─────────────┬──────┬─────────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n",
            "│     ┆ Monit_MAP_C ┆ Year ┆ LAKE_NAME   ┆ … ┆ Percentage_ ┆ Percentage_ ┆ Split_Grou ┆ Split_Grou │\n",
            "│ --- ┆ ODE1        ┆ ---  ┆ ---         ┆   ┆ Other_Heati ┆ No_Heating  ┆ p          ┆ p_right    │\n",
            "│ i64 ┆ ---         ┆ i64  ┆ str         ┆   ┆ ng          ┆ ---         ┆ ---        ┆ ---        │\n",
            "│     ┆ str         ┆      ┆             ┆   ┆ ---         ┆ f64         ┆ str        ┆ str        │\n",
            "│     ┆             ┆      ┆             ┆   ┆ f64         ┆             ┆            ┆            │\n",
            "╞═════╪═════════════╪══════╪═════════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n",
            "│ 0   ┆ 02000500-01 ┆ 2014 ┆ George      ┆ … ┆ 85.325639   ┆ 14.674361   ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "│ 1   ┆ 02000500-01 ┆ 2013 ┆ George      ┆ … ┆ 1.154163    ┆ 0.0         ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "│ 2   ┆ 02000500-01 ┆ 2012 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "│ 3   ┆ 02000500-01 ┆ 2011 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "│ 4   ┆ 02000500-01 ┆ 2010 ┆ George      ┆ … ┆ 0.0         ┆ 0.0         ┆ Test       ┆ Test       │\n",
            "│     ┆             ┆      ┆ Watch Lake  ┆   ┆             ┆             ┆            ┆            │\n",
            "└─────┴─────────────┴──────┴─────────────┴───┴─────────────┴─────────────┴────────────┴────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aa2ed74",
        "outputId": "d9f74d3c-a4a1-4afd-f2f5-3a529dd0a5a0"
      },
      "source": [
        "# water_quality = water_quality.drop('Split_Group_right') # This line caused the error, so it's removed.\n",
        "\n",
        "# Verify and display the distribution of lakes across the three sets\n",
        "split_distribution = water_quality['Split_Group'].value_counts(normalize=True)\n",
        "# Polars' value_counts(normalize=True) returns a DataFrame with 'proportion' column\n",
        "split_distribution = split_distribution.with_columns(\n",
        "    (pl.col('proportion') * 100).round(2).alias('percentage')\n",
        ").select(pl.col('Split_Group'), pl.col('percentage'))\n",
        "\n",
        "print(\"Distribution of samples across split groups:\")\n",
        "print(split_distribution)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution of samples across split groups:\n",
            "shape: (3, 2)\n",
            "┌─────────────┬────────────┐\n",
            "│ Split_Group ┆ percentage │\n",
            "│ ---         ┆ ---        │\n",
            "│ str         ┆ f64        │\n",
            "╞═════════════╪════════════╡\n",
            "│ Test        ┆ 22.0       │\n",
            "│ Validation  ┆ 33.0       │\n",
            "│ Training    ┆ 45.0       │\n",
            "└─────────────┴────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbc76ef8",
        "outputId": "fbcfac16-d9c6-4134-cce6-52812b360c98"
      },
      "source": [
        "split_distribution = (\n",
        "    water_quality['Split_Group']\n",
        "    .value_counts(normalize=True)\n",
        "    .with_columns(\n",
        "        (pl.col('proportion') * 100).round(2).alias('percentage')\n",
        "    )\n",
        "    .select(pl.col('Split_Group'), pl.col('percentage'))\n",
        ")\n",
        "print(\"Distribution of samples across split groups:\")\n",
        "print(split_distribution)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution of samples across split groups:\n",
            "shape: (3, 2)\n",
            "┌─────────────┬────────────┐\n",
            "│ Split_Group ┆ percentage │\n",
            "│ ---         ┆ ---        │\n",
            "│ str         ┆ f64        │\n",
            "╞═════════════╪════════════╡\n",
            "│ Validation  ┆ 33.0       │\n",
            "│ Test        ┆ 22.0       │\n",
            "│ Training    ┆ 45.0       │\n",
            "└─────────────┴────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "# Assuming water_quality is a Polars DataFrame\n",
        "split_distribution = (\n",
        "    water_quality['Split_Group']                  # Select the column as a Series\n",
        "    .value_counts(sort=True, normalize=True)      # Get counts + proportion (normalized fraction)\n",
        "    .with_columns(\n",
        "        (pl.col(\"proportion\") * 100)               # Use 'proportion' column\n",
        "        .round(2)\n",
        "        .alias(\"percentage\")\n",
        "    )\n",
        "    .select(\"Split_Group\", \"percentage\")      # Keep only what you want\n",
        ")\n",
        "\n",
        "print(\"Distribution of samples across split groups:\")\n",
        "print(split_distribution)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5gs4yOg2F7L",
        "outputId": "064b76ed-f7af-4228-8f3c-1ab3880372d7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution of samples across split groups:\n",
            "shape: (3, 2)\n",
            "┌─────────────┬────────────┐\n",
            "│ Split_Group ┆ percentage │\n",
            "│ ---         ┆ ---        │\n",
            "│ str         ┆ f64        │\n",
            "╞═════════════╪════════════╡\n",
            "│ Training    ┆ 45.0       │\n",
            "│ Validation  ┆ 33.0       │\n",
            "│ Test        ┆ 22.0       │\n",
            "└─────────────┴────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeb0bebf",
        "outputId": "8535a808-e5d9-4dac-f3ed-6d0dc1c74f84"
      },
      "source": [
        "split_distribution = (\n",
        "    water_quality['Split_Group']\n",
        "    .value_counts(normalize=True)\n",
        "    .with_columns(\n",
        "        (pl.col('proportion') * 100).round(2).alias('percentage')\n",
        "    )\n",
        "    .select(pl.col('Split_Group'), pl.col('percentage'))\n",
        ")\n",
        "print(\"Distribution of samples across split groups:\")\n",
        "print(split_distribution)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution of samples across split groups:\n",
            "shape: (3, 2)\n",
            "┌─────────────┬────────────┐\n",
            "│ Split_Group ┆ percentage │\n",
            "│ ---         ┆ ---        │\n",
            "│ str         ┆ f64        │\n",
            "╞═════════════╪════════════╡\n",
            "│ Training    ┆ 45.0       │\n",
            "│ Test        ┆ 22.0       │\n",
            "│ Validation  ┆ 33.0       │\n",
            "└─────────────┴────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58e3ee1c"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "Yes, the split was successfully created. 10 unique lakes were split into 'Training' (5 lakes, 50%), 'Validation' (3 lakes, 30%), and 'Test' (2 lakes, 20%) sets. A `Split_Group` column was added to the `water_quality` DataFrame. The distribution of records (samples) in the `water_quality` DataFrame across these groups is 45.0% for Training, 22.0% for Test, and 33.0% for Validation.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A total of 10 unique lakes were identified in the `water_quality` DataFrame.\n",
        "*   These unique lakes were successfully split into:\n",
        "    *   Training set: 5 lakes, representing 50% of unique lakes.\n",
        "    *   Validation set: 3 lakes, representing 30% of unique lakes.\n",
        "    *   Test set: 2 lakes, representing 20% of unique lakes.\n",
        "*   A new `Split_Group` column was successfully added to the `water_quality` DataFrame, assigning each record to its respective 'Training', 'Validation', or 'Test' group based on its `LAKE_NAME`.\n",
        "*   The distribution of *samples* (records) within the `water_quality` DataFrame across the split groups is:\n",
        "    *   Training: 45.0%\n",
        "    *   Test: 22.0%\n",
        "    *   Validation: 33.0%\n",
        "    *   These percentages differ slightly from the initial lake split percentages (50%, 20%, 30%) because the number of data points per lake in the `water_quality` DataFrame varies.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The established lake-based split ensures that all data points for a given lake are consistently in one set, which is crucial for preventing data leakage when modeling, especially if lake-specific features or patterns are significant.\n",
        "*   While the sample distribution slightly deviates from the target lake distribution, this is expected given the varying number of records per lake. Further analysis could explore whether this sample distribution bias significantly impacts model training or evaluation, or if a different splitting strategy (e.g., balancing records per lake) is required for specific modeling objectives.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13ad99e0"
      },
      "source": [
        "# Task 2\n",
        "\n",
        "For each target,\n",
        "\n",
        "1. Perform a grid search for both CART and Random Forests using the training lakes,\n",
        "\n",
        "2. Compare the best model of each type on the test lakes to determine the best overall model.\n",
        "\n",
        "3. Refit the best model on the 70% of the lakes not in the validation set (training + test).\n",
        "\n",
        "4. Use the validation set to estimate the performance of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea1f3bd0",
        "outputId": "86ad0319-ab86-49c5-f43d-72b7c6169f72"
      },
      "source": [
        "print(water_quality.columns)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Monit_MAP_CODE1', 'Year', 'LAKE_NAME', 'Mean_Secchi_Depth_Result', 'Mean_Phosporus_Result', 'Mean_EMV_Total', 'STD_EMV_Total', 'Mean_Sale_Value', 'STD_Sale_Value', 'Mean_Total_Tax', 'STD_Total_Tax', 'Mean_Garage_Size', 'STD_Garage_Size', 'Mean_Fin_SQ_FT', 'STD_Fin_SQ_FT', 'Percentage_Yes_Basement', 'Percentage_Yes_Garage', 'Percentage_Yes_Tax_Exempt', 'Percentage_Air_Cooling', 'Percentage_AC_Cooling', 'Percentage_Central_Cooling', 'Percentage_Other_Cooling', 'Percentage_No_Cooling', 'Percentage_Air_Heating', 'Percentage_Space_Heating', 'Percentage_Water_Heating', 'Percentage_Electric_Heating', 'Percentage_Other_Heating', 'Percentage_No_Heating', 'Split_Group']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b38cf57",
        "outputId": "f44613d8-4e66-4ef5-f0f3-cad85a84d3dd"
      },
      "source": [
        "target_variables = [\n",
        "    'Mean_Secchi_Depth_Result',\n",
        "    'Mean_Phosporus_Result'\n",
        "]\n",
        "\n",
        "print(f\"Selected target variables: {target_variables}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected target variables: ['Mean_Secchi_Depth_Result', 'Mean_Phosporus_Result']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e92da5bb",
        "outputId": "9ff35fab-8375-499a-d904-a93e2edb66f3"
      },
      "source": [
        "train_df = water_quality.filter(pl.col('Split_Group') == 'Training')\n",
        "val_df = water_quality.filter(pl.col('Split_Group') == 'Validation')\n",
        "test_df = water_quality.filter(pl.col('Split_Group') == 'Test')\n",
        "\n",
        "print(f\"Shape of training DataFrame: {train_df.shape}\")\n",
        "print(f\"Shape of validation DataFrame: {val_df.shape}\")\n",
        "print(f\"Shape of test DataFrame: {test_df.shape}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training DataFrame: (45, 31)\n",
            "Shape of validation DataFrame: (33, 31)\n",
            "Shape of test DataFrame: (22, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "154763a0",
        "outputId": "42aadc63-a791-4dd1-b088-907838b2468e"
      },
      "source": [
        "feature_cols = [col for col in train_df.columns if col not in ['LAKE_NAME', 'Monit_MAP_CODE1', 'Split_Group'] + target_variables]\n",
        "\n",
        "print(f\"Number of feature columns: {len(feature_cols)}\")\n",
        "print(\"First 5 feature columns:\")\n",
        "print(feature_cols[:5])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of feature columns: 26\n",
            "First 5 feature columns:\n",
            "['', 'Year', 'Mean_EMV_Total', 'STD_EMV_Total', 'Mean_Sale_Value']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dd780fe",
        "outputId": "e76bd065-6e06-43ae-8b73-e1c21d7618aa"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for DecisionTreeRegressor\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "print(\"Imported DecisionTreeRegressor and GridSearchCV.\")\n",
        "print(f\"Parameter grid for DecisionTreeRegressor: {param_grid}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported DecisionTreeRegressor and GridSearchCV.\n",
            "Parameter grid for DecisionTreeRegressor: {'max_depth': [3, 5, 7, 10], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6060fc13",
        "outputId": "d860eb51-9d20-42bc-c2b3-ce0fc77a0e5c"
      },
      "source": [
        "best_cart_models = {}\n",
        "\n",
        "for target in target_variables:\n",
        "    print(f\"\\nPerforming GridSearchCV for target: {target}\")\n",
        "\n",
        "    # Prepare X_train and y_train\n",
        "    X_train = train_df.select(feature_cols).to_pandas()\n",
        "    y_train = train_df.select(target).to_pandas().squeeze()\n",
        "\n",
        "    # Instantiate DecisionTreeRegressor\n",
        "    dt = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator\n",
        "    best_cart_models[target] = grid_search.best_estimator_\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Best parameters for {target}: {grid_search.best_params_}\")\n",
        "    print(f\"Best R-squared score for {target}: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\nGrid search for CART models completed.\")\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing GridSearchCV for target: Mean_Secchi_Depth_Result\n",
            "Best parameters for Mean_Secchi_Depth_Result: {'max_depth': 7, 'min_samples_leaf': 1, 'min_samples_split': 10}\n",
            "Best R-squared score for Mean_Secchi_Depth_Result: -0.8559\n",
            "\n",
            "Performing GridSearchCV for target: Mean_Phosporus_Result\n",
            "Best parameters for Mean_Phosporus_Result: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
            "Best R-squared score for Mean_Phosporus_Result: -0.2085\n",
            "\n",
            "Grid search for CART models completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01f0ed19",
        "outputId": "619b3ba9-f9b0-4595-b440-6303a02c79d3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Define the parameter grid for RandomForestRegressor\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "print(\"Imported RandomForestRegressor.\")\n",
        "print(f\"Parameter grid for RandomForestRegressor: {param_grid_rf}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported RandomForestRegressor.\n",
            "Parameter grid for RandomForestRegressor: {'n_estimators': [50, 100, 150], 'max_depth': [5, 10, 15], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea8d99b4",
        "outputId": "d905fa4b-b99c-4d21-806a-b9d8b40c795d"
      },
      "source": [
        "best_rf_models = {}\n",
        "\n",
        "for target in target_variables:\n",
        "    print(f\"\\nPerforming GridSearchCV for Random Forest for target: {target}\")\n",
        "\n",
        "    # Prepare X_train and y_train\n",
        "    X_train = train_df.select(feature_cols).to_pandas()\n",
        "    y_train = train_df.select(target).to_pandas().squeeze()\n",
        "\n",
        "    # Instantiate RandomForestRegressor\n",
        "    rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best estimator\n",
        "    best_rf_models[target] = grid_search_rf.best_estimator_\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Best parameters for {target}: {grid_search_rf.best_params_}\")\n",
        "    print(f\"Best R-squared score for {target}: {grid_search_rf.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\nGrid search for Random Forest models completed.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing GridSearchCV for Random Forest for target: Mean_Secchi_Depth_Result\n",
            "Best parameters for Mean_Secchi_Depth_Result: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150}\n",
            "Best R-squared score for Mean_Secchi_Depth_Result: -0.5668\n",
            "\n",
            "Performing GridSearchCV for Random Forest for target: Mean_Phosporus_Result\n",
            "Best parameters for Mean_Phosporus_Result: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Best R-squared score for Mean_Phosporus_Result: -0.6480\n",
            "\n",
            "Grid search for Random Forest models completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70515b91",
        "outputId": "e004910b-9d16-4836-bb6d-12b7deaf61e8"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Imported evaluation metrics.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported evaluation metrics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38575916",
        "outputId": "de1b6a01-7f1a-40e7-c048-6731d0e6faf9"
      },
      "source": [
        "model_performance = {}\n",
        "\n",
        "# Prepare X_test once as features are the same for all targets\n",
        "X_test = test_df.select(feature_cols).to_pandas()\n",
        "\n",
        "for target in target_variables:\n",
        "    print(f\"\\n--- Evaluating models for target: {target} ---\")\n",
        "\n",
        "    # Prepare y_test for the current target\n",
        "    y_test = test_df.select(target).to_pandas().squeeze()\n",
        "\n",
        "    # --- Evaluate CART Model ---\n",
        "    cart_model = best_cart_models[target]\n",
        "    cart_predictions = cart_model.predict(X_test)\n",
        "\n",
        "    cart_r2 = r2_score(y_test, cart_predictions)\n",
        "    cart_mae = mean_absolute_error(y_test, cart_predictions)\n",
        "    cart_rmse = np.sqrt(mean_squared_error(y_test, cart_predictions))\n",
        "\n",
        "    print(f\"CART Model Performance for {target}:\")\n",
        "    print(f\"  R-squared: {cart_r2:.4f}\")\n",
        "    print(f\"  MAE: {cart_mae:.4f}\")\n",
        "    print(f\"  RMSE: {cart_rmse:.4f}\")\n",
        "\n",
        "    model_performance[f'CART_{target}'] = {\n",
        "        'R-squared': cart_r2,\n",
        "        'MAE': cart_mae,\n",
        "        'RMSE': cart_rmse\n",
        "    }\n",
        "\n",
        "    # --- Evaluate Random Forest Model ---\n",
        "    rf_model = best_rf_models[target]\n",
        "    rf_predictions = rf_model.predict(X_test)\n",
        "\n",
        "    rf_r2 = r2_score(y_test, rf_predictions)\n",
        "    rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
        "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
        "\n",
        "    print(f\"Random Forest Model Performance for {target}:\")\n",
        "    print(f\"  R-squared: {rf_r2:.4f}\")\n",
        "    print(f\"  MAE: {rf_mae:.4f}\")\n",
        "    print(f\"  RMSE: {rf_rmse:.4f}\")\n",
        "\n",
        "    model_performance[f'RF_{target}'] = {\n",
        "        'R-squared': rf_r2,\n",
        "        'MAE': rf_mae,\n",
        "        'RMSE': rf_rmse\n",
        "    }\n",
        "\n",
        "    # --- Compare Models ---\n",
        "    if cart_r2 > rf_r2:\n",
        "        print(f\"Conclusion for {target}: CART model performed better (higher R-squared).\")\n",
        "    else:\n",
        "        print(f\"Conclusion for {target}: Random Forest model performed better (higher R-squared).\")\n",
        "\n",
        "print(\"\\n--- Overall Model Performance Summary ---\")\n",
        "print(model_performance)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating models for target: Mean_Secchi_Depth_Result ---\n",
            "CART Model Performance for Mean_Secchi_Depth_Result:\n",
            "  R-squared: -58.8026\n",
            "  MAE: 1.2843\n",
            "  RMSE: 1.3376\n",
            "Random Forest Model Performance for Mean_Secchi_Depth_Result:\n",
            "  R-squared: -46.1389\n",
            "  MAE: 1.1524\n",
            "  RMSE: 1.1875\n",
            "Conclusion for Mean_Secchi_Depth_Result: Random Forest model performed better (higher R-squared).\n",
            "\n",
            "--- Evaluating models for target: Mean_Phosporus_Result ---\n",
            "CART Model Performance for Mean_Phosporus_Result:\n",
            "  R-squared: -2.9490\n",
            "  MAE: 0.1584\n",
            "  RMSE: 0.1893\n",
            "Random Forest Model Performance for Mean_Phosporus_Result:\n",
            "  R-squared: -2.7465\n",
            "  MAE: 0.1617\n",
            "  RMSE: 0.1843\n",
            "Conclusion for Mean_Phosporus_Result: Random Forest model performed better (higher R-squared).\n",
            "\n",
            "--- Overall Model Performance Summary ---\n",
            "{'CART_Mean_Secchi_Depth_Result': {'R-squared': -58.802590720814166, 'MAE': 1.2843351919545098, 'RMSE': np.float64(1.337569785541465)}, 'RF_Mean_Secchi_Depth_Result': {'R-squared': -46.1388791902032, 'MAE': 1.152350441585835, 'RMSE': np.float64(1.1875340657871567)}, 'CART_Mean_Phosporus_Result': {'R-squared': -2.94897055535826, 'MAE': 0.15840054349690713, 'RMSE': np.float64(0.18925486845503106)}, 'RF_Mean_Phosporus_Result': {'R-squared': -2.7464575835263836, 'MAE': 0.16170727118374362, 'RMSE': np.float64(0.18433827642092002)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "552838f8",
        "outputId": "c26e4083-4c70-4c2b-8aed-0a9781798523"
      },
      "source": [
        "refitted_best_models = {}\n",
        "\n",
        "# Combine train_df and test_df\n",
        "combined_train_test_df = pl.concat([train_df, test_df])\n",
        "\n",
        "for target in target_variables:\n",
        "    print(f\"\\nRefitting best model for target: {target}\")\n",
        "\n",
        "    # Prepare X_combined and y_combined\n",
        "    X_combined = combined_train_test_df.select(feature_cols).to_pandas()\n",
        "    y_combined = combined_train_test_df.select(target).to_pandas().squeeze()\n",
        "\n",
        "    # Retrieve the best model (Random Forest, as it performed better)\n",
        "    best_model = best_rf_models[target]\n",
        "\n",
        "    # Refit the model on the combined dataset\n",
        "    best_model.fit(X_combined, y_combined)\n",
        "\n",
        "    # Store the refitted model\n",
        "    refitted_best_models[target] = best_model\n",
        "\n",
        "    print(f\"Model for {target} refitted successfully.\")\n",
        "\n",
        "print(\"\\nRefitting of best models completed.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Refitting best model for target: Mean_Secchi_Depth_Result\n",
            "Model for Mean_Secchi_Depth_Result refitted successfully.\n",
            "\n",
            "Refitting best model for target: Mean_Phosporus_Result\n",
            "Model for Mean_Phosporus_Result refitted successfully.\n",
            "\n",
            "Refitting of best models completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2002895",
        "outputId": "7e26ccc3-a8bd-47ed-9ab1-ded39a59880d"
      },
      "source": [
        "validation_performance = {}\n",
        "\n",
        "# Prepare X_val once as features are the same for all targets\n",
        "X_val = val_df.select(feature_cols).to_pandas()\n",
        "\n",
        "for target in target_variables:\n",
        "    print(f\"\\n--- Evaluating refitted model for target on Validation Set: {target} ---\")\n",
        "\n",
        "    # Prepare y_val for the current target\n",
        "    y_val = val_df.select(target).to_pandas().squeeze()\n",
        "\n",
        "    # Retrieve the refitted model\n",
        "    refitted_model = refitted_best_models[target]\n",
        "\n",
        "    # Make predictions on X_val\n",
        "    val_predictions = refitted_model.predict(X_val)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    val_r2 = r2_score(y_val, val_predictions)\n",
        "    val_mae = mean_absolute_error(y_val, val_predictions)\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Refitted Model Performance on Validation Set for {target}:\")\n",
        "    print(f\"  R-squared: {val_r2:.4f}\")\n",
        "    print(f\"  MAE: {val_mae:.4f}\")\n",
        "    print(f\"  RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "    # Store metrics\n",
        "    validation_performance[f'Refitted_RF_{target}'] = {\n",
        "        'R-squared': val_r2,\n",
        "        'MAE': val_mae,\n",
        "        'RMSE': val_rmse\n",
        "    }\n",
        "\n",
        "print(\"\\n--- Overall Refitted Model Performance on Validation Set ---\")\n",
        "print(validation_performance)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating refitted model for target on Validation Set: Mean_Secchi_Depth_Result ---\n",
            "Refitted Model Performance on Validation Set for Mean_Secchi_Depth_Result:\n",
            "  R-squared: 0.0437\n",
            "  MAE: 0.7147\n",
            "  RMSE: 0.8284\n",
            "\n",
            "--- Evaluating refitted model for target on Validation Set: Mean_Phosporus_Result ---\n",
            "Refitted Model Performance on Validation Set for Mean_Phosporus_Result:\n",
            "  R-squared: -2.3472\n",
            "  MAE: 0.0706\n",
            "  RMSE: 0.0740\n",
            "\n",
            "--- Overall Refitted Model Performance on Validation Set ---\n",
            "{'Refitted_RF_Mean_Secchi_Depth_Result': {'R-squared': 0.04366748426061484, 'MAE': 0.7146721920909161, 'RMSE': np.float64(0.828413304201175)}, 'Refitted_RF_Mean_Phosporus_Result': {'R-squared': -2.347249729895068, 'MAE': 0.07060640639692127, 'RMSE': np.float64(0.07396098370408526)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5857859"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "Summarize the entire process and what we learned\n",
        "\n",
        "## Overall Summary\n",
        "\n",
        "In this project we tried to predict two water quality indicators, Mean Secchi Depth and Mean Phosphorus, using tree-based models. The dataset was split into training, validation, and test groups, but the models struggled to make accurate predictions. On the training data, both CART and Random Forest models had negative R-squared values, meaning they performed worse than just guessing the average. When tested, Random Forest did slightly better than CART, but still had very poor results with large negative R-squared scores. Even after refitting the best Random Forest models, performance on the validation set was weak, with one target showing only a very small positive R-squared and the other still negative. Overall, this shows that the current features don't explain the water quality measures well, and the models can't capture the relationships in the data. To improve, we'll need stronger feature engineering, maybe adding time-series information, and trying more advanced algorithms like XGBoost or LightGBM that might handle complex patterns better.\n"
      ]
    }
  ]
}